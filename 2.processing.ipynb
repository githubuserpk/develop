{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38147777-a93d-4954-9476-10b776cf1dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# --------CREATE Table\n",
    "# ---------------------\n",
    "\n",
    "table_name = \"testing.bronze.t1\"\n",
    "\n",
    "if not spark.catalog.tableExists(table_name):\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            id INT,\n",
    "            name STRING\n",
    "        )\n",
    "        USING DELTA\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d5ab51-a9fc-4fcb-aa1a-2ff2420484ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# --------DROP Table\n",
    "# ---------------------\n",
    "\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Table name\n",
    "table_name = \"testing.bronze.t1\"\n",
    "\n",
    "# Step 1: Check if table exists\n",
    "if spark.catalog.tableExists(table_name):\n",
    "    print(f\"✅ Table {table_name} exists. Proceeding to drop and clean.\")\n",
    "\n",
    "    # Step 2: Get Delta table location from DESCRIBE DETAIL\n",
    "    try:\n",
    "        location_row = spark.sql(f\"DESCRIBE DETAIL {table_name}\").collect()[0]\n",
    "        table_location = location_row[\"location\"]\n",
    "        print(f\"📁 Table data location found: {table_location}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to get table location: {e}\")\n",
    "        table_location = None\n",
    "\n",
    "    # Step 3: Drop the table\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    print(f\"🗑️ Dropped table: {table_name}\")\n",
    "\n",
    "    if table_location:\n",
    "        # Step 4: Disable Delta retention safety check\n",
    "        spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "        print(\"⛔ Retention check disabled temporarily.\")\n",
    "\n",
    "        # Step 5: Force vacuum (delete files immediately)\n",
    "        spark.sql(f\"VACUUM delta.`{table_location}` RETAIN 0 HOURS\")\n",
    "        print(\"🧹 Vacuum complete. Obsolete files removed.\")\n",
    "\n",
    "        # Step 6: Re-enable Delta retention check\n",
    "        spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "        print(\"🔒 Retention check re-enabled for safety.\")\n",
    "else:\n",
    "    print(f\"ℹ️ Table {table_name} does not exist. Nothing to drop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d2cfb4a-0329-4613-b5a2-556de6c60ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Step 1: Get next ID as before\n",
    "if spark.catalog.tableExists(\"testing.bronze.t1\"):\n",
    "    max_id_row = spark.sql(\"SELECT MAX(id) as max_id FROM testing.bronze.t1\").collect()[0]\n",
    "    current_max_id = max_id_row[\"max_id\"] or 0\n",
    "else:\n",
    "    current_max_id = 0\n",
    "\n",
    "next_id = current_max_id + 1\n",
    "name = f\"Product {next_id}\"\n",
    "\n",
    "# Step 2: Create an explicit schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Step 3: Create DataFrame with defined schema\n",
    "data = [(next_id, name)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Step 4: Append to table\n",
    "df.write.mode(\"append\").saveAsTable(\"testing.bronze.t1\")\n",
    "\n",
    "print(f\"✅ Inserted row: id={next_id}, name='{name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ac7aa0e-45f9-4eee-b0de-dfec0131ea3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from testing.bronze.t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570a5658-dddc-4728-90df-621899576273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widgets\n",
    "dbutils.widgets.text(\"start_date\", \"2025-07-01\")\n",
    "dbutils.widgets.text(\"end_date\", \"2025-07-10\")\n",
    "dbutils.widgets.text(\"product_id\", \"BREAD\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c70493-16aa-4707-831d-d3ab6896a000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get widget values\n",
    "start_date = dbutils.widgets.get(\"start_date\")\n",
    "end_date = dbutils.widgets.get(\"end_date\")\n",
    "product_id = dbutils.widgets.get(\"product_id\")\n",
    "\n",
    "print(f\"product_id: {product_id}\")\n",
    "print(f\"start_date: {start_date}\")\n",
    "print(f\"end_date: {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64335a89-b814-4e13-a91f-ff2a693d55e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col, lit, length\n",
    "\n",
    "# Widgets\n",
    "dbutils.widgets.text(\"start_date\", \"2025-07-01\")\n",
    "dbutils.widgets.text(\"end_date\", \"2025-07-10\")\n",
    "dbutils.widgets.text(\"product_id\", \"BREAD\")\n",
    "\n",
    "# Get widget values\n",
    "start_date = dbutils.widgets.get(\"start_date\")\n",
    "end_date = dbutils.widgets.get(\"end_date\")\n",
    "product_id = dbutils.widgets.get(\"product_id\")\n",
    "\n",
    "# Read from source\n",
    "raw_df = spark.table(\"samples.bakehouse.sales_transactions\")\n",
    "\n",
    "# Step 1: Filter rows where `dateTime` is non-empty and matches expected date format\n",
    "# Adjust the regex to match your actual format. Example below assumes `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss`\n",
    "filtered_raw_df = raw_df.filter(\n",
    "    (col(\"dateTime\").isNotNull()) &\n",
    "    (length(col(\"dateTime\")) > 0) &\n",
    "    (col(\"dateTime\").rlike(\"^[0-9]{4}-[0-9]{2}-[0-9]{2}.*\"))  # Basic format check\n",
    ")\n",
    "\n",
    "# Step 2: Safely cast to date after filtering\n",
    "df = (\n",
    "    filtered_raw_df\n",
    "    .withColumn(\"sale_date\", to_date(col(\"dateTime\")))\n",
    "    .filter(col(\"sale_date\").isNotNull())\n",
    "    .filter(\n",
    "        (col(\"sale_date\") >= to_date(lit(start_date))) &\n",
    "        (col(\"sale_date\") <= to_date(lit(end_date))) &\n",
    "        (col(\"product\") == lit(product_id))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 3: Target table setup\n",
    "target_table = \"testing.bronze.pk_salestxn\"\n",
    "if not spark.catalog.tableExists(target_table):\n",
    "    df.limit(0).write.mode(\"overwrite\").saveAsTable(target_table)\n",
    "\n",
    "# Step 4: Write data\n",
    "df.write.mode(\"append\").saveAsTable(target_table)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7204662821586074,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.processing",
   "widgets": {
    "end_date": {
     "currentValue": "2025-01-31",
     "nuid": "0eabd576-e4e2-4a48-810e-cbcbb83b1334",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-07-10",
      "label": null,
      "name": "end_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-07-10",
      "label": null,
      "name": "end_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "product_id": {
     "currentValue": "D0100",
     "nuid": "33afd0ae-dbe4-4b35-9a14-428c77f2eaf4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "BREAD",
      "label": null,
      "name": "product_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "BREAD",
      "label": null,
      "name": "product_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "start_date": {
     "currentValue": "2025-01-01",
     "nuid": "b169f558-baf1-46d2-a41b-454ee8ab25bd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-07-01",
      "label": null,
      "name": "start_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-07-01",
      "label": null,
      "name": "start_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
